{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import imageio.v3 as iio\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import display, Image, Video\n",
    "\n",
    "from ipywebrtc import CameraStream, ImageRecorder, VideoRecorder\n",
    "camera = CameraStream(constraints={\n",
    "    \"facing_mode\": \"user\",\n",
    "    \"audio\": False,\n",
    "    \"video\": { \"width\": 640, \"height\": 480 }\n",
    "\n",
    "})\n",
    "\n",
    "\n",
    "recorder = ImageRecorder(stream=camera)\n",
    "\n",
    "stop_button = widgets.Button(description=\"Stop Camera\", icon=\"stop\", button_style=\"danger\")\n",
    "stop_button.on_click(lambda _: camera.close())\n",
    "\n",
    "widgets.VBox([ widgets.HBox([ camera, recorder ]), stop_button ])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict\n",
    "from LE.model_no_cord import UNetModelFlatten\n",
    "import torch\n",
    "import dsntnn\n",
    "device = torch.device('mps')\n",
    "model = UNetModelFlatten(UNetModelFlatten(4, 3, 20, 64).to(device))\n",
    "model.load_state_dict(torch.load(\"./flatten4\"))\n",
    "model.eval()\n",
    "\n",
    "prediction= model(recorder.image.value)\n",
    "unnomilizedpred=dsntnn.normalized_to_pixel_coordinates(prediction,(480,640))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "#from LE.testUnet import UNetModel\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.multiclass import OneVsOneClassifier, OneVsRestClassifier\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "numpy_data = unnomilizedpred.numpy()\n",
    "distance_matrix = np.zeros((20,20))\n",
    "\n",
    "for i in range(len(numpy_data)):\n",
    "    #Check that the joint is visible\n",
    "    if(numpy_data[i][0] != 0 and numpy_data[i][1] != 0):\n",
    "        #For each other joint in the frame\n",
    "        for j in range(len(numpy_data)):\n",
    "            #Check that the joint is visible\n",
    "            if(numpy_data[i][0] != 0 and numpy_data[i][0] != 0):\n",
    "                #Calculate distance between joint i and all joints j and add to matrix\n",
    "                distance_matrix[i,j] = math.sqrt((numpy_data[i][0] - numpy_data[j][0])**2 + (numpy_data[i][1] - numpy_data[j][1])**2)\n",
    "\n",
    "#Flatten list \n",
    "flattened_distances = np.reshape(distance_matrix, (1,400))\n",
    "\n",
    "\n",
    "#Get the actual gesture based on prediction number\n",
    "gestures = [\"ASL_letter_A\", \"ASL_letter_B\", \"ASL_letter_C\", \"ASL_letter_L\", \"ASL_letter_R\", \"ASL_letter_U\",\n",
    "\"ASL_letter_G\", \"ASL_letter_H\", \"ASL_letter_I\", \"ASL_letter_V\", \"ASL_letter_W\", \"ASL_letter_Y\"]\n",
    "\n",
    "# Load model\n",
    "file = open('finalized_SVM_model.sav', 'rb')\n",
    "GR_model = pickle.load(file)\n",
    "#print(GR_model.config)\n",
    "\n",
    "pred = GR_model.predict(flattened_distances)\n",
    "\n",
    "print(\"The predicted gesture from input data is:\", gestures[int(pred[0])])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
