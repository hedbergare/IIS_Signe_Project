{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "592232c6a5294bdf9a3b0cb6cf7b4bff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(CameraStream(constraints={'facing_mode': 'user', 'audio': False, 'video': {'widtâ€¦"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import imageio.v3 as iio\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import display, Image, Video\n",
    "\n",
    "from ipywebrtc import CameraStream, ImageRecorder, VideoRecorder\n",
    "camera = CameraStream(constraints={\n",
    "    \"facing_mode\": \"user\",\n",
    "    \"audio\": False,\n",
    "    \"video\": { \"width\": 640, \"height\": 480 }\n",
    "\n",
    "})\n",
    "\n",
    "\n",
    "recorder = ImageRecorder(stream=camera)\n",
    "\n",
    "stop_button = widgets.Button(description=\"Stop Camera\", icon=\"stop\", button_style=\"danger\")\n",
    "stop_button.on_click(lambda _: camera.close())\n",
    "\n",
    "widgets.VBox([ widgets.HBox([ camera, recorder ]), stop_button ])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "tensor([[[139., 143., 135.,  ..., 107., 116., 113.],\n",
      "         [119., 119., 117.,  ..., 112.,  90.,  72.],\n",
      "         [ 97.,  77.,  63.,  ...,  51.,  46.,  35.],\n",
      "         ...,\n",
      "         [152., 153., 148.,  ..., 148., 143., 140.],\n",
      "         [146., 142., 139.,  ..., 136., 104.,  88.],\n",
      "         [140., 106.,  90.,  ..., 172., 161., 160.]],\n",
      "\n",
      "        [[150., 157., 155.,  ..., 152., 155., 150.],\n",
      "         [154., 154., 150.,  ..., 146., 144., 140.],\n",
      "         [148., 147., 143.,  ..., 136.,  97.,  83.],\n",
      "         ...,\n",
      "         [170.,  94.,  70.,  ..., 244., 179., 174.],\n",
      "         [204., 144., 135.,  ..., 163., 113., 112.],\n",
      "         [163., 113., 112.,  ..., 191., 174., 168.]],\n",
      "\n",
      "        [[135.,  78.,  63.,  ..., 166.,  93.,  68.],\n",
      "         [166.,  94.,  71.,  ..., 255., 203., 197.],\n",
      "         [226., 164., 158.,  ..., 162., 117., 114.],\n",
      "         ...,\n",
      "         [ 44.,  37.,  51.,  ...,  46.,  31.,  27.],\n",
      "         [ 47.,  34.,  31.,  ...,  18.,  10.,  15.],\n",
      "         [ 18.,  10.,  15.,  ...,  49.,  39.,  49.]]])\n",
      "tensor([[237.0713, 425.2032],\n",
      "        [221.4720, 273.5020],\n",
      "        [370.1882, 238.4395],\n",
      "        [329.1258, 195.5848],\n",
      "        [281.4189, 233.7045],\n",
      "        [367.3125, 246.1351],\n",
      "        [390.0887, 183.3060],\n",
      "        [201.0252, 211.5319],\n",
      "        [339.9035, 191.2303],\n",
      "        [366.8582, 215.5591],\n",
      "        [429.9874, 222.9813],\n",
      "        [382.0730, 193.6441],\n",
      "        [334.8134, 198.6269],\n",
      "        [231.7961, 174.5025],\n",
      "        [368.7149, 285.7171],\n",
      "        [283.5402, 240.1307],\n",
      "        [395.1380, 264.3384],\n",
      "        [275.9652, 228.2291],\n",
      "        [341.4247, 163.1197],\n",
      "        [322.0601, 258.3264]], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#predict\n",
    "from LE.model_no_cord import UNetModelFlatten\n",
    "import torch\n",
    "import dsntnn\n",
    "snapshot = iio.imread(recorder.image.value, mode=\"RGB\")\n",
    "\n",
    "\n",
    "snap=torch.tensor(snapshot)\n",
    "snap =torch.reshape(snap,(3,640,480))\n",
    "snap=snap.float()\n",
    "print(type(snap[0][0][0]))\n",
    "print(snap)\n",
    "model = UNetModelFlatten(4, 3, 20, 64)\n",
    "model.load_state_dict(torch.load(\"./flatten4.pth\",map_location=torch.device('cpu')))\n",
    "model.eval()\n",
    "\n",
    "prediction= model(snap)\n",
    "unnomilizedpred=dsntnn.normalized_to_pixel_coordinates(prediction,(480,640))\n",
    "print(unnomilizedpred)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted gesture from input data is: ASL_letter_C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dmkslice/.local/lib/python3.8/site-packages/sklearn/base.py:288: UserWarning: Trying to unpickle estimator SVC from version 1.1.3 when using version 1.2.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/dmkslice/.local/lib/python3.8/site-packages/sklearn/base.py:288: UserWarning: Trying to unpickle estimator OneVsOneClassifier from version 1.1.3 when using version 1.2.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "#from LE.testUnet import UNetModel\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.multiclass import OneVsOneClassifier, OneVsRestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def normalize_by_joint_distance(X_values):\n",
    "    for i in range(len(X_values)):\n",
    "        distance = 0\n",
    "        # At least one of the following joint distances exists for all frames!\n",
    "        if (X_values[i][6][7] != 0):\n",
    "            distance = X_values[i][6][7]\n",
    "\n",
    "        elif (X_values[i][10][11] != 0):\n",
    "            distance = X_values[i][10][11]\n",
    "\n",
    "        elif (X_values[i][14][15] != 0):\n",
    "            distance = X_values[i][14][15]\n",
    "\n",
    "        elif (X_values[i][18][19] != 0):\n",
    "            distance = X_values[i][18][19]\n",
    "\n",
    "        elif (X_values[i][2][3] != 0):\n",
    "            distance = X_values[i][2][3]\n",
    "\n",
    "        X_values[i] = X_values[i] / distance\n",
    "    return X_values\n",
    "\n",
    "\n",
    "numpy_data = unnomilizedpred.detach().numpy()\n",
    "distance_matrix = np.zeros((20,20))\n",
    "for i in range(len(numpy_data)):\n",
    "    #Check that the joint is visible\n",
    "    if(numpy_data[i][0] != 0 and numpy_data[i][1] != 0):\n",
    "        #For each other joint in the frame\n",
    "        for j in range(len(numpy_data)):\n",
    "            #Check that the joint is visible\n",
    "            if(numpy_data[i][0] != 0 and numpy_data[i][0] != 0):\n",
    "                #Calculate distance between joint i and all joints j and add to matrix\n",
    "                distance_matrix[i,j] = math.sqrt((numpy_data[i][0] - numpy_data[j][0])**2 + (numpy_data[i][1] - numpy_data[j][1])**2)\n",
    "\n",
    "#Flatten list \n",
    "distance_matrix=normalize_by_joint_distance([distance_matrix])\n",
    "flattened_distances = np.reshape(distance_matrix, (1,400))\n",
    "\n",
    "\n",
    "\n",
    "#Get the actual gesture based on prediction number\n",
    "gestures = [\"ASL_letter_A\", \"ASL_letter_B\", \"ASL_letter_C\", \"ASL_letter_L\", \"ASL_letter_R\", \"ASL_letter_U\",\n",
    "\"ASL_letter_G\", \"ASL_letter_H\", \"ASL_letter_I\", \"ASL_letter_V\", \"ASL_letter_W\", \"ASL_letter_Y\"]\n",
    "\n",
    "# Load model\n",
    "file = open('finalized_linear_model.sav', 'rb')\n",
    "GR_model = pickle.load(file)\n",
    "#print(GR_model.config)\n",
    "\n",
    "pred = GR_model.predict(flattened_distances)\n",
    "\n",
    "print(\"The predicted gesture from input data is:\", gestures[int(pred[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
